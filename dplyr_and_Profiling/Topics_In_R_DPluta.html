<!DOCTYPE html>
<html>
  <head>
    <title>Topics in R Workshop</title>
    <meta charset="utf-8">
    <meta name="author" content="Dustin Pluta" />
    <link href="libs/remark-css/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle

# Topics in R Workshop
## UCI Data Science Initiative
### Dustin Pluta
### 2 Dec 2016

---


![](Figures/UCI_logo.png)

![](Figures/dsi_logo.png)





---
# Topics in R: Afternoon Schedule

12:45 - 2:30pm: &lt;code&gt;dplyr&lt;/code&gt;

2:40 - 3:50pm: Profiling and vectorisation

![](Figures/kiwi.jpg)

---
# Required Packages
Install needed packages:

```r
# setwd(~/source/file/location/)
install.packages('ggplot2')
install.packages('dplyr')
install.packages('broom')
install.packages('microbenchmark')
install.packages('devtools')
devtools::install_github('hadley/lineprof')
```


```r
library(ggplot2)
library(dplyr)
library(broom)
library(microbenchmark)
library(lineprof)
```


---

# Topics


1. Using &lt;code&gt;dplyr&lt;/code&gt; Functions

--

2. Examples with &lt;code&gt;%&gt;%&lt;/code&gt;

--

3. Performant Code

--

4. Vectorisation

--

5. Profiling


---

# Some Resources for R

* [&lt;code&gt;dplyr&lt;/code&gt; and Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

* [Data Carpentry Lessons for R](http://www.datacarpentry.org/R-ecology-lesson/)

* [&lt;code&gt;dplyr&lt;/code&gt; Tutorial](http://genomicsclass.github.io/book/pages/dplyr_tutorial.html)

* [Advanced R](http://adv-r.had.co.nz/)

* [R for Data Science](http://r4ds.had.co.nz/)

* [Coursera Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)

---

# &lt;code&gt;dplyr&lt;/code&gt;

* &lt;code&gt;dplyr&lt;/code&gt; is a package designed for easy and efficient data manipulation

* Fast enough to work with large(ish) data sets

* *tidyverse* philosophy: collection of small, simple functions that each do one thing well

* Written by Hadley Wickham, Chief Scientist for R Studio, who also developed:

    + &lt;code&gt;ggplot2&lt;/code&gt;

    + &lt;code&gt;reshape2&lt;/code&gt;

    + &lt;code&gt;tidyr &lt;/code&gt;

    + many others
    
* **Note** Because Dr. Wickham is from New Zealand, we will be strictly using 
New Zealand spellings (colour, summarise, vectorisation, etc.) here.


---
# Key Functions

* &lt;code&gt; filter&lt;/code&gt;: select subset of rows (observations)

* &lt;code&gt; select&lt;/code&gt;: select subset of columns (variables)

* &lt;code&gt; mutate&lt;/code&gt;: transform variables in a data set

* &lt;code&gt; arrange&lt;/code&gt;: reorder rows

* &lt;code&gt; summarise&lt;/code&gt;: collapses a data frame into a single row

* &lt;code&gt;group_by&lt;/code&gt;

![](Figures/iris.png)

---
# Simple &lt;code&gt;dplyr&lt;/code&gt; Examples
* Let's try some &lt;code&gt;dplyr&lt;/code&gt; functions with the &lt;code&gt;iris&lt;/code&gt; data set:

```r
# Manipulate the data.frame iris and print species means of Sepal Width
data(iris)
iris &lt;-  filter(iris, Species!="setosa")
iris &lt;- select(iris, c(Sepal.Width, Species)) 
iris &lt;- group_by(iris, Species)
species_means &lt;- summarise(iris, mean(Sepal.Width))
print(species_means)
```

```
## # A tibble: 2 × 2
##      Species `mean(Sepal.Width)`
##       &lt;fctr&gt;               &lt;dbl&gt;
## 1 versicolor               2.770
## 2  virginica               2.974
```

* Note that each function has the form &lt;code&gt;f(data, ...)&lt;/code&gt;: first 
argument is a data frame, second argument indicates what 
to do with the data frame.
* Each function is designed to perform a single specific type of operation on a data frame.
* Data can be cleaned and organized by chaining these operations together.

---
# Introducing &lt;code&gt;%&gt;%&lt;/code&gt;
* &lt;code&gt;dplyr&lt;/code&gt; (and much of the *tidyverse*) is designed around the use of 
the pipe operator &lt;code&gt;%&gt;%&lt;/code&gt;

* The pipe operator &lt;code&gt;%&gt;%&lt;/code&gt; allows you to chain operations on a data set together 
without having to create specific intermediate objects

* When using &lt;code&gt;%&gt;%&lt;/code&gt;, the first argument to a function is taken as the output of the previous step in the chain

* For example, the following is equivalent to the previous code:

```r
# Prints species means, does not save anything
# Original data.frame iris is unaffected
data(iris)
iris %&gt;% filter(Species!="setosa") %&gt;%
    select(c(Sepal.Width, Species)) %&gt;% 
    group_by(Species) %&gt;%
    summarise(mean(Sepal.Width))
```

```
## # A tibble: 2 × 2
##      Species `mean(Sepal.Width)`
##       &lt;fctr&gt;               &lt;dbl&gt;
## 1 versicolor               2.770
## 2  virginica               2.974
```


```r
# To save the results instead
species_means &lt;- iris %&gt;% 
    filter(Species!="setosa") %&gt;%
    select(c(Sepal.Width, Species)) %&gt;% 
    group_by(Species) %&gt;%
    summarise(mean(Sepal.Width))
```

---
# &lt;code&gt;dplyr&lt;/code&gt; Exercises

1. Use &lt;code&gt;dplyr&lt;/code&gt; to calculate the mean Sepal Width of the virginica species.

2. &lt;code&gt;summarise&lt;/code&gt; can summarise multiple variables simultaneously, applying a (possibly different) 
function to each variable.  
Adapt the code below to find the minimum, median, 
maximum, and standard deviation of the Sepal.Width for the virginica species.

3. &lt;code&gt;group_by()&lt;/code&gt; makes &lt;code&gt;summarise&lt;/code&gt; even more useful by allowing you 
to summarise values across groups of a category simultaneously.  
Using &lt;code&gt;group_by&lt;/code&gt;, adapt your code from the previous problem to produce the summary values for each species.


**Modify this code for problems 2 and 3:**

```r
data(iris)
iris %&gt;% summarise(mean_sepal_width = mean(Sepal.Width),
                   min_sepal_width = min(Sepal.Width))
```

---
# &lt;code&gt;dplyr&lt;/code&gt; Exercises
*Solution*
* 1. Use &lt;code&gt;dplyr&lt;/code&gt; to calculate the mean Sepal Width of the virginica species.


```r
data(iris)

iris %&gt;% 
    filter(Species == "virginica") %&gt;%
    summarise(mean_sepal_width = mean(Sepal.Width))
```

```
##   mean_sepal_width
## 1            2.974
```


---
# &lt;code&gt;dplyr&lt;/code&gt; Exercises
*Solution*

* 2. &lt;code&gt;summarise&lt;/code&gt; can summarise multiple variables simultaneously, applying a (possibly different) 
function to each variable.  
Adapt the code below to find the minimum, median, 
maximum, and standard deviation of the Sepal.Width for the virginica species.



```r
data(iris)
iris %&gt;% 
    filter(Species == "virginica") %&gt;%
    summarise(min_sepal_width = min(Sepal.Width),
              med = median(Sepal.Width), maximum = max(Sepal.Width),
              stdev = sd(Sepal.Width))
```

```
##   min_sepal_width med maximum     stdev
## 1             2.2   3     3.8 0.3224966
```


---
# &lt;code&gt;dplyr&lt;/code&gt; Exercises
*Solution*

* 3. &lt;code&gt;group_by()&lt;/code&gt; makes &lt;code&gt;summarise&lt;/code&gt; even more useful by allowing you 
to summarise values across groups of a category simultaneously.  
Using &lt;code&gt;group_by&lt;/code&gt;, adapt your code from the previous problem to produce the summary values for each species.



```r
data(iris)
iris %&gt;% 
    group_by(Species) %&gt;%
    summarise(min_sepal_width = min(Sepal.Width),
              med = median(Sepal.Width), maximum = max(Sepal.Width),
              stdev = sd(Sepal.Width))
```

```
## # A tibble: 3 × 5
##      Species min_sepal_width   med maximum     stdev
##       &lt;fctr&gt;           &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
## 1     setosa             2.3   3.4     4.4 0.3790644
## 2 versicolor             2.0   2.8     3.4 0.3137983
## 3  virginica             2.2   3.0     3.8 0.3224966
```


---
class: center, middle
# Performant Code

&gt;“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.”
~ Donald Knuth.

![](Figures/young-donald-knuth.jpg)


---
# Performant Code

* When writing code, there are many different resources to consider:
    + Speed
    + Memory Usage
    + Developer Time
    + Code Reusability

* We generally want to make the data analysis process easy, convenient, and reliable, 
it's rarely necessary to write the fastest code possible.

* Goal: learn easy methods and tools to reduce the time and burden of programming 
for data analysis.
    + Vectorisation
    + Profiling tools
    + Design principles

---
# Performant Code

**Procedures** (from [Advanced R](adv-r.had.co.na/Profiling.html#improve-perf])):

1. Identify the bottlenecks in a script with profiling tools.
    + &lt;code&gt;microbenchmark&lt;/code&gt;
    + &lt;code&gt;lineprof&lt;/code&gt;
2. Look for existing solutions.
    + Published packages
    + StackOverflow
3. Vectorise.
    + &lt;code&gt; apply, lapply &lt;/code&gt;
    + Specialized functions, e.g. &lt;code&gt;colSums, rowSums&lt;/code&gt;
4. Parallelise.
5. Avoid copying code uncessarily; avoid superfluous copies of data and variables.
6. Byte-code compile.
7. Rewrite in a faster language (C++, Rcpp, Julia).


---
# Vectorisation

* R is **bad** with "native" &lt;code&gt;for&lt;/code&gt; loops, due to various aspects of 
R's design

* Vectorised functions in R tend to be much faster than equivalent loops.

* The underlying loops of the vectorised fucntions are implemented in a lower level 
language like C, which can greatly improve runtimes.

* Key functions:
    + **&lt;code&gt;apply(X, MARGIN, FUN)&lt;/code&gt;:** Apply function FUN to matrix/array X, across the dimension(s) specified by MARGIN.
    
    + **&lt;code&gt;sapply(X, FUN)&lt;/code&gt;:** Apply a function to a list of vectors, lists, or similar, and returns a vector or matrix by default.

---
# Vectorisation: Iris Example

Suppose we want to calculate the mean of all the numeric variables in the &lt;code&gt;iris&lt;/code&gt; 
data set.


```r
data(iris)
head(iris)
```

```
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
```

```r
# With a loop:
iris_means &lt;- c()
for (i in 1:4) {
    iris_means[i] &lt;- mean(iris[, i])
}

# Vectorised:
iris_means &lt;- apply(iris[, -5], 2, mean)
```

---
# Vectorisation: Iris Example
Let's use vectorisation to calculate 95% bootstrap confidence intervals for the 
median Sepal.Length for each iris species.

**Part A**
First, use &lt;code&gt;group_by&lt;/code&gt; and &lt;code&gt;summarise&lt;/code&gt; to calculate the median Sepal Length of each species.  Then plot density/histogram plots of the distributions 
of Sepal Lengths for the three species using &lt;code&gt;ggplot&lt;/code&gt;.

---
# Vectorisation: Iris Example
**Part A**
*Solution*


```r
iris_medians &lt;- group_by(iris, Species) %&gt;%
    summarise(median(Sepal.Length))

ggplot(iris, aes(Sepal.Length, fill=Species)) + 
    geom_density(alpha = 0.3) + 
    geom_vline(aes(xintercept = iris_medians[, 2]))
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-12-1.png" width="672" /&gt;

---
# Vectorisation: Iris Example
**Part B**

* Suppose we want to print the lower and upper 95\% bootstrap estimates, along with the sample median Sepal Length.  

    + Clean up and rewrite the following bootstrap code using vectorisation and &lt;code&gt;dplyr&lt;/code&gt; functions. 
    
    (Hint: the &lt;code&gt;boostrap()&lt;/code&gt; function from &lt;code&gt;broom&lt;/code&gt; makes bootstrap sampling easy.)

---
# Vectorisation: Iris Example
**Part B**

Modify this code using &lt;code&gt;dplyr&lt;/code&gt; functions.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- subset(iris, Species == "setosa")
boot_samples &lt;- matrix(nrow=B, ncol=n)
boot_medians &lt;- c()
for (b in 1:B) {
    boot_samples[b, ] &lt;- sample_n(setosa, n, replace=TRUE)[, "Sepal.Length"] 
    boot_medians[b] &lt;- median(boot_samples[b, ])
}

boot_median_est &lt;- mean(boot_medians)
boot_lower &lt;- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper &lt;- boot_medians[order(boot_medians)][round(0.975*B)]
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Use the &lt;code&gt;boostrap()&lt;/code&gt; function.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- subset(iris, Species == "setosa")
boot_samples &lt;- matrix(nrow=B, ncol=n)
boot_medians &lt;- c()

boot_samples &lt;- bootstrap(setosa, B)



boot_median_est &lt;- mean(boot_medians)
boot_lower &lt;- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper &lt;- boot_medians[order(boot_medians)][round(0.975*B)]
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Replace &lt;code&gt;subset&lt;/code&gt; with &lt;code&gt;filter&lt;/code&gt;, and remove extraneous declarations.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- filter(iris, Species == "setosa")

boot_samples &lt;- bootstrap(setosa, B)
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate medians for each replicate using &lt;code&gt;do&lt;/code&gt; and &lt;code&gt;tidy&lt;/code&gt;.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- filter(iris, Species == "setosa")

boot_samples &lt;- bootstrap(setosa, B)
boot_medians &lt;- boot_samples %&gt;% do(tidy(median(.$Sepal.Length)))
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate bootstrap confidence bounds for the median Sepal Length.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- filter(iris, Species == "setosa")

boot_samples &lt;- bootstrap(setosa, B)
boot_medians &lt;- boot_samples %&gt;% do(tidy(median(.$Sepal.Length)))
boot_bounds &lt;- quantile(boot_medians$x, probs=c(0.025, 0.975))

cat("Est Median Sepal Length: ", mean(boot_medians$x), "\n", 
    "95% Bootstrap CI for Median Sepal Length: (", 
    boot_bounds[1], ",", boot_bounds[2], ")\n")
```

```
## Est Median Sepal Length:  5.01195 
##  95% Bootstrap CI for Median Sepal Length: ( 4.9 , 5.1 )
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate bootstrap confidence bounds for the median Sepal Length.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
boot_medians &lt;- iris %&gt;% 
    filter(Species == "setosa") %&gt;%
    bootstrap(B) %&gt;% 
    do(tidy(median(.$Sepal.Length)))
boot_bounds &lt;- quantile(boot_medians$x, probs=c(0.025, 0.975))
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Compare back to what we started with.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa &lt;- subset(iris, Species == "setosa")
boot_samples &lt;- matrix(nrow=B, ncol=n)
boot_medians &lt;- c()
for (b in 1:B) {
    boot_samples[b, ] &lt;- sample_n(setosa, n, replace=TRUE)[, "Sepal.Length"] 
    boot_medians[b] &lt;- median(boot_samples[b, ])
}

boot_median_est &lt;- mean(boot_medians)
boot_lower &lt;- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper &lt;- boot_medians[order(boot_medians)][round(0.975*B)]
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

&lt;code&gt;dplyr&lt;/code&gt; version again, now with printing for results.

```r
data(iris)
set.seed(1234)
B &lt;- 1000 # Number of bootstrap samples
n &lt;- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
boot_medians &lt;- iris %&gt;% 
    filter(Species == "setosa") %&gt;%
    bootstrap(B) %&gt;% 
    do(tidy(median(.$Sepal.Length)))
boot_bounds &lt;- quantile(boot_medians$x, probs=c(0.025, 0.975))

cat("Est Median Sepal Length: ", mean(boot_medians$x), "\n", 
    "95% Bootstrap CI for Median Sepal Length: (", 
    boot_bounds[1], ",", boot_bounds[2], ")\n")
```

```
## Est Median Sepal Length:  5.01195 
##  95% Bootstrap CI for Median Sepal Length: ( 4.9 , 5.1 )
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*


```r
ggplot(boot_medians, aes(x)) + 
    geom_histogram(binwidth=0.05)
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-21-1.png" width="672" /&gt;

*End iris example*

---
# Profiling

**&lt;code&gt;lineprof&lt;/code&gt;** is a package for profiling R scripts.

Here is an example on a simple function, from [Advanced R](adv-r/had/co.nz/Profiling.html)


```r
library(lineprof)
f &lt;- function() {
  pause(0.1)
  g()
  h()
}
g &lt;- function() {
  pause(0.1)
  h()
}
h &lt;- function() {
  pause(0.1)
}

L &lt;- lineprof(f())
L
```

```
##    time alloc release dups                 ref         src
## 1 0.024 0.001       0    2 c("pause", ".Call") pause/.Call
## 2 0.025 0.001       0    0            &lt;text&gt;#8 g/pause    
## 3 0.025 0.001       0    0            &lt;text&gt;#9 g/h        
## 4 0.025 0.000       0    0           &lt;text&gt;#12 h/pause
```

---
# Profiling


```r
# lm runs too fast on small data to test with lineprof
data(iris)
lineprof(lm(Sepal.Length ~ Species, data=iris))
```


```r
# Simulate some bigger data to test lm
N &lt;- 1E4
beta &lt;- c(1, 0, 2, -3, 4)
X &lt;- as.matrix(rnorm(N*length(beta), 10, 1), nrow=N)
Y &lt;- X%*%beta + rnorm(N, 0, 2)
L &lt;- lineprof(lm(Y ~ X))
L
```

```
##    time alloc release dups                                       ref
## 1 0.003 0.390   0.134   73                         c("eval", "eval")
## 2 0.014 3.584   0.426   16                          "model.response"
## 3 0.003 0.046   0.000   59 c("model.matrix", "model.matrix.default")
## 4 0.005 0.217   0.047    4                                  "lm.fit"
## 5 0.013 0.000   0.002    2                    c("lm.fit", "rep.int")
##                                 src
## 1 eval/eval                        
## 2 model.response                   
## 3 model.matrix/model.matrix.default
## 4 lm.fit                           
## 5 lm.fit/rep.int
```

---
# Profiling
* The profiling package &lt;code&gt;microbenchmark&lt;/code&gt; can also be used to profile 
small pieces of code.

* &lt;code&gt;microbenchmark&lt;/code&gt; will run input code many times to try to get stable 
estimates of run times, thus it can take very long to run for complicated or long-running 
functions.


```r
library(microbenchmark)
microbenchmark(lm(Sepal.Length ~ Species, data=iris))
```

```
## Unit: milliseconds
##                                     expr      min      lq     mean
##  lm(Sepal.Length ~ Species, data = iris) 1.238497 1.28434 1.366687
##    median       uq      max neval
##  1.306315 1.323873 3.512007   100
```

```r
microbenchmark(lm(Y ~ X))
```

```
## Unit: milliseconds
##       expr     min       lq     mean   median       uq      max neval
##  lm(Y ~ X) 61.8195 122.2756 120.9616 122.4824 122.7499 162.1283   100
```

---
# Profiling: Vectorisation

Let's compare the speeds of for-loops and the equivalent vectorisations.


```r
set.seed(1234)
n &lt;- 100
a &lt;- rnorm(n)
b &lt;- rexp(n, 5)

dot_loop &lt;- function(a, b) {
    result &lt;- 0
    for (i in 1:n)
        result &lt;- result + a[i]*b[i]
    return(result)
}
dot_loop(a, b)
```

```
## [1] -5.083198
```

```r
a%*%b
```

```
##           [,1]
## [1,] -5.083198
```

---
# Profiling: Vectorisation

```r
microbenchmark(dot_loop(a, b))
```

```
## Unit: microseconds
##            expr    min       lq     mean   median       uq     max neval
##  dot_loop(a, b) 98.038 105.4485 107.4147 106.4825 107.6585 143.651   100
```

```r
microbenchmark(a%*%b)
```

```
## Unit: microseconds
##     expr   min    lq    mean median     uq   max neval
##  a %*% b 1.071 1.085 1.18313  1.091 1.1325 7.452   100
```

---
# Profiling: Vectorisation

```r
set.seed(1234)
n &lt;- 1E5
a &lt;- rnorm(n, 10, 2)
b &lt;- rexp(n, 5)

microbenchmark(dot_loop(a, b))
```

```
## Unit: milliseconds
##            expr      min       lq     mean   median       uq      max
##  dot_loop(a, b) 46.64226 48.03896 49.29673 48.42731 48.70397 92.72951
##  neval
##    100
```

```r
microbenchmark(a%*%b)
```

```
## Unit: microseconds
##     expr     min      lq     mean  median      uq     max neval
##  a %*% b 706.807 706.947 711.1084 707.076 708.833 751.873   100
```

---
# Profiling: Vectorisation

```r
# This size may take a minute or two to finish
set.seed(1234)
n &lt;- 1E6
a &lt;- rnorm(n, 10, 2)
b &lt;- rexp(n, 5)

microbenchmark(dot_loop(a, b))
microbenchmark(a%*%b)
```

---
# Profiling: Vector Sum Example

* Let's compare summing a vector with a for-loop vs using &lt;code&gt;sum&lt;/code&gt;.

* Write two functions to return the sum of all entries of an input vector *x*: 
    
    + &lt;code&gt;loop_sum(x)&lt;/code&gt; using a for-loop
    
    + &lt;code&gt;vect_sum(x)&lt;/code&gt; using vectorisation (i.e., with &lt;code&gt;sum()&lt;/code&gt;)


---
# Profiling: Vector Sum Example
*Solution*

* Let's compare summing a vector with &lt;code&gt;sum&lt;/code&gt; vs a for-loop.

* Write two functions to return the sum of all entries of an input vector *x*: 
   
    + &lt;code&gt;loop_sum(x)&lt;/code&gt; using a for-loop
   
    + &lt;code&gt;vect_sum(x)&lt;/code&gt; using vectorised functions (Hint: &lt;code&gt;sum&lt;/code&gt;)


```r
loop_sum &lt;- function(x) {
    result &lt;- 0
    for (i in 1:length(x)) {
        result &lt;- result + x[i]
    }
    return(result)
}

vect_sum &lt;- function(x) {
    return(sum(x))
}
```

---
# Profiling: Vector Sum Example

```r
set.seed(1234)
x &lt;- rnorm(500)

microbenchmark(loop_sum(x))
```

```
## Unit: microseconds
##         expr     min       lq    mean   median       uq     max neval
##  loop_sum(x) 165.522 173.6225 181.797 179.3765 186.2475 245.135   100
```

```r
microbenchmark(vect_sum(x))
```

```
## Unit: nanoseconds
##         expr min    lq    mean median     uq  max neval
##  vect_sum(x) 863 916.5 1107.77 1019.5 1110.5 9059   100
```

---
# Profiling: &lt;code&gt;aggregate&lt;/code&gt; vs &lt;code&gt;dplyr&lt;/code&gt;

Let's compare using &lt;code&gt;aggregate&lt;/code&gt; to &lt;code&gt;dplyr&lt;/code&gt; for 
finding the standard deviation of Sepal Length for each of the iris species.


```r
data(iris)

microbenchmark(aggregate(Sepal.Length ~ Species, data=iris, FUN=mean))
```

```
## Unit: microseconds
##                                                        expr     min
##  aggregate(Sepal.Length ~ Species, data = iris, FUN = mean) 754.332
##       lq     mean   median       uq      max neval
##  807.185 850.8007 820.5655 850.0055 2395.399   100
```

```r
microbenchmark(iris %&gt;% 
                   group_by(Species) %&gt;% 
                   summarise(mean(Sepal.Length)))
```

```
## Unit: microseconds
##                                                          expr     min
##  iris %&gt;% group_by(Species) %&gt;% summarise(mean(Sepal.Length)) 718.794
##       lq     mean  median      uq      max neval
##  754.794 793.1708 780.215 794.076 2104.708   100
```

```r
microbenchmark(iris %&gt;% 
                   select(Sepal.Length, Species) %&gt;%
                   group_by(Species) %&gt;% 
                   summarise(mean(Sepal.Length)))
```

```
## Unit: microseconds
##                                                                                                 expr
##  iris %&gt;% select(Sepal.Length, Species) %&gt;% group_by(Species) %&gt;%      summarise(mean(Sepal.Length))
##      min       lq     mean median      uq      max neval
##  857.575 904.4675 941.6546 931.44 951.572 2053.935   100
```

---
# Profiling: &lt;code&gt;aggregate&lt;/code&gt; vs &lt;code&gt;dplyr&lt;/code&gt;


```r
country &lt;- read.csv("Data/countryLandTemperaturesByCountry.csv")

microbenchmark(aggregate(AverageTemperature ~ Country, data=country, FUN=mean))
microbenchmark(country %&gt;% group_by(Country) %&gt;% 
                   summarise(mean(AverageTemperature)))
```

---
# Profiling: Iris Example

To profile the previous code from the Iris Example, we wrap everything 
in a function:

```r
set.seed(1234)
data(iris)

iris_loop &lt;- function(B, n=50,
                           species="setosa", var="Sepal.Length") {
    iris_species &lt;- subset(iris, Species == species)
    boot_samples &lt;- matrix(nrow=B, ncol=n)
    boot_medians &lt;- c()
    for (b in 1:B) {
        boot_samples[b, ] &lt;- sample_n(setosa, n, replace=TRUE)[, var] 
        boot_medians[b] &lt;- median(boot_samples[b, ])
    }
    boot_median_est &lt;- mean(boot_medians)
    boot_lower &lt;- boot_medians[order(boot_medians)][round(0.025*B)]
    boot_upper &lt;- boot_medians[order(boot_medians)][round(0.975*B)]
    return(c(boot_median_est, boot_lower, boot_upper))
}
iris_loop(50)
```

```
## [1] 5.013 4.900 5.100
```

---
# Profiling: Iris Example

```r
microbenchmark(iris_loop(300))
lineprof(iris_loop(1000))
```

---
# Profiling: Iris Example

Now for the &lt;code&gt;dplyr&lt;/code&gt;.

```r
set.seed(1234)
iris_vect &lt;- function(B, n=50, species="setosa") {
    boot_medians &lt;- iris %&gt;% 
        filter(Species == species) %&gt;%
        bootstrap(B) %&gt;% 
        do(tidy(median(.$Sepal.Length)))
    boot_bounds &lt;- quantile(boot_medians$x, probs=c(0.025, 0.975))
    return(c(mean(boot_medians$x), boot_bounds))
}
iris_vect(50)
```

```
##            2.5%   97.5% 
## 5.01300 4.91125 5.10000
```

---
# Profiling: Iris Example

Comparing the two versions for the bootstrap iris estimates.


```r
microbenchmark(iris_loop(300))
```

```
## Unit: milliseconds
##            expr      min       lq     mean   median       uq      max
##  iris_loop(300) 55.34436 56.93437 57.24776 57.08744 57.39887 62.48432
##  neval
##    100
```

```r
microbenchmark(iris_vect(300))
```

```
## Unit: milliseconds
##            expr      min       lq     mean   median       uq      max
##  iris_vect(300) 88.40634 92.80448 96.02314 95.40859 97.66792 139.8333
##  neval
##    100
```


---
# Profiling: Iris Example

Comparing the two versions for the bootstrap iris estimates.


```r
lineprof(iris_loop(2000))
```

```
##     time  alloc release  dups                                  ref
## 1  0.002  3.870   0.000  2577 c("sample_n", "sample_n.data.frame")
## 2  0.001  1.879   0.000  1105        c("median", "median.default")
## 3  0.003  5.819   0.000  3296 c("sample_n", "sample_n.data.frame")
## 4  0.002  4.119   0.000  2302        c("median", "median.default")
## 5  0.010 17.339  28.351 10485 c("sample_n", "sample_n.data.frame")
## 6  0.001  2.136   0.000  1217               c("[", "[.data.frame")
## 7  0.006 12.131   0.000  7074 c("sample_n", "sample_n.data.frame")
## 8  0.001  2.011   0.000  1098                                  "["
## 9  0.002  4.172   0.000  2352 c("sample_n", "sample_n.data.frame")
## 10 0.001  1.674   0.000  1190        c("median", "median.default")
## 11 0.004  6.426  27.859  4264 c("sample_n", "sample_n.data.frame")
## 12 0.001  2.133   0.000  1222        c("median", "median.default")
## 13 0.005 10.402   0.000  5976 c("sample_n", "sample_n.data.frame")
## 14 0.001  2.097   0.000  1183               c("[", "[.data.frame")
## 15 0.002  4.087   0.000  2349 c("sample_n", "sample_n.data.frame")
## 16 0.001  2.053   0.000  1181                             "median"
## 17 0.002  2.173  28.351  1787 c("sample_n", "sample_n.data.frame")
## 18 0.003  6.471   0.000  3703        c("median", "median.default")
## 19 0.007 14.457   0.000  8279 c("sample_n", "sample_n.data.frame")
## 20 0.001  2.087   0.000  1179               c("[", "[.data.frame")
## 21 0.001  2.087   0.000  1190                           "sample_n"
## 22 0.005  8.261  28.245  5439 c("sample_n", "sample_n.data.frame")
## 23 0.001  2.150   0.000  1161        c("median", "median.default")
## 24 0.003  6.228   0.000  3613 c("sample_n", "sample_n.data.frame")
## 25 0.001  2.097   0.000  1147        c("median", "median.default")
## 26 0.002  4.129   0.000  2341 c("sample_n", "sample_n.data.frame")
## 27 0.001  2.107   0.000  1195                         character(0)
## 28 0.003  4.158  28.470  3547 c("sample_n", "sample_n.data.frame")
## 29 0.001  2.087   0.000   638                         character(0)
## 30 0.001  2.008   0.000  1184        c("median", "median.default")
## 31 0.003  6.384   0.000  3540 c("sample_n", "sample_n.data.frame")
## 32 0.001  2.104   0.000  1217               c("[", "[.data.frame")
## 33 0.002  4.136   0.000  2376 c("sample_n", "sample_n.data.frame")
## 34 0.001  2.102   0.000  1159        c("median", "median.default")
## 35 0.004  8.292   0.000  4721 c("sample_n", "sample_n.data.frame")
## 36 0.001  1.959   0.000  1161                         character(0)
## 37 0.004  6.464  28.680  4096 c("sample_n", "sample_n.data.frame")
## 38 0.001  2.161   0.000  1225                         character(0)
## 39 0.001  2.103   0.000  1218               c("[", "[.data.frame")
## 40 0.001  2.153   0.000  1190                         character(0)
## 41 0.004  6.219   0.000  4718 c("sample_n", "sample_n.data.frame")
##                             src
## 1  sample_n/sample_n.data.frame
## 2  median/median.default       
## 3  sample_n/sample_n.data.frame
## 4  median/median.default       
## 5  sample_n/sample_n.data.frame
## 6  [/[.data.frame              
## 7  sample_n/sample_n.data.frame
## 8  [                           
## 9  sample_n/sample_n.data.frame
## 10 median/median.default       
## 11 sample_n/sample_n.data.frame
## 12 median/median.default       
## 13 sample_n/sample_n.data.frame
## 14 [/[.data.frame              
## 15 sample_n/sample_n.data.frame
## 16 median                      
## 17 sample_n/sample_n.data.frame
## 18 median/median.default       
## 19 sample_n/sample_n.data.frame
## 20 [/[.data.frame              
## 21 sample_n                    
## 22 sample_n/sample_n.data.frame
## 23 median/median.default       
## 24 sample_n/sample_n.data.frame
## 25 median/median.default       
## 26 sample_n/sample_n.data.frame
## 27                             
## 28 sample_n/sample_n.data.frame
## 29                             
## 30 median/median.default       
## 31 sample_n/sample_n.data.frame
## 32 [/[.data.frame              
## 33 sample_n/sample_n.data.frame
## 34 median/median.default       
## 35 sample_n/sample_n.data.frame
## 36                             
## 37 sample_n/sample_n.data.frame
## 38                             
## 39 [/[.data.frame              
## 40                             
## 41 sample_n/sample_n.data.frame
```

```r
lineprof(iris_vect(2000))
```

```
##    time   alloc release   dups                     ref             src
## 1 0.152 267.343 270.252 121994 c("%&gt;%", "withVisible") %&gt;%/withVisible
```

---
# Profiling Exercise

1. Suppose we want to find the column sums of the numeric variables in the &lt;code&gt;iris&lt;/code&gt; data set.  
Use &lt;code&gt;microbenchmark&lt;/code&gt; to compare the speed of three methods for computing 
the column sums:
    
    + double for-loop over columns and rows
    
    + &lt;code&gt;apply&lt;/code&gt; and &lt;code&gt;sum&lt;/code&gt;
    
    + &lt;code&gt;colSums&lt;/code&gt;
    
2. Simulate a random matrix (say 100 x 100 or more), and benchmark the different column 
sum methods.

---
# Profiling Exercise
*Solution*


```r
data(iris)

loop &lt;- function(dat) {
    sums &lt;- c()
    for (j in 1:4) {
        result &lt;- 0
        for (i in nrow(dat)) {
            result &lt;- result + iris[i, j]
        }
        sums &lt;- c(sums, result)
    }
    return(sums)
}

my_apply &lt;- function(dat) {
   return(apply(dat, 2, sum))
}

my_colsums &lt;- function(dat)
    return(colSums(dat))
```

---
# Profiling Exercise
*Solution*


```r
microbenchmark(loop(iris[, -5]))
```

```
## Unit: microseconds
##              expr    min      lq     mean  median     uq     max neval
##  loop(iris[, -5]) 64.932 69.3735 79.73028 72.5725 81.203 350.261   100
```

```r
microbenchmark(my_apply(iris[, -5]))
```

```
## Unit: microseconds
##                  expr    min     lq     mean median      uq     max neval
##  my_apply(iris[, -5]) 73.969 76.074 82.60574 77.641 87.1875 189.013   100
```

```r
microbenchmark(my_colsums(iris[, -5]))
```

```
## Unit: microseconds
##                    expr    min      lq     mean  median     uq     max
##  my_colsums(iris[, -5]) 48.679 50.7005 56.93548 51.9815 60.905 234.824
##  neval
##    100
```

---
# Profiling Exercise
*Solution*


```r
set.seed(1234)
n_col &lt;- 100
n_row &lt;- 100
X &lt;- matrix(rnorm(n_col*n_row), nrow=n_row)

microbenchmark(loop(X))
```

```
## Unit: microseconds
##     expr    min     lq     mean median      uq     max neval
##  loop(X) 43.068 45.873 51.92589 47.028 56.6485 103.194   100
```

```r
microbenchmark(my_apply(X))
```

```
## Unit: microseconds
##         expr     min       lq    mean  median       uq      max neval
##  my_apply(X) 189.141 195.6435 229.091 203.157 205.8945 2724.385   100
```

```r
microbenchmark(my_colsums(X))
```

```
## Unit: microseconds
##           expr    min    lq     mean  median      uq    max neval
##  my_colsums(X) 11.565 11.92 13.19363 12.1005 12.3205 31.942   100
```

---
# Summary

* Vectorization and good code design and save you lots of time in the long run, and 
gets much easier with practice.

* When applicable, vectorization tends to be much faster than for-loops

* However, some care must be taken in correctly implementing the vectorized code

* Loops may make more sense when
    + using functions that do not take vectors as input
    + computations where one iteration depends on previous iterations

* If you can't use built-in vertorised functions and have to use a loop, 
the package &lt;code&gt;Rcpp&lt;/code&gt; can be used to write fast for-loops in C++.

---
# Summary

* There is a nice R-bloggers post on [Strategies to Speedup R Code](https://www.r-bloggers.com/strategies-to-speedup-r-code/), 
which includes many plots showing relative speedups for different 
strategies.

&lt;img src="Figures/raw_vs_with_vectorization.png" alt="vectorization" style="width: 600px;"/&gt;


---
# Language Speed Comparisons
* For situations where speed and efficiency is critical, you will likely need to 
resort to other languages:

&lt;img src="Figures/julia_benchmarks.png" alt="Language Comparisons" style="width: 700px;"/&gt;


---
# Analysing Climate Data

* Let's apply some of the ideas we've seen to more complicated real-world data sets.

* This is a more open-ended section, to give you an opportunity to explore and practice 
using &lt;code&gt;dplyr&lt;/code&gt;, vectorisation, and profiling.

* Download climate data *GlobalTemperatures.csv*, which consists of temperature readings globally over time.

* Download climate data *GlobalLandTemperaturesByCountry.csv*, which consists of temperature readings over time by country.

* Download climate data *GlobalLandTemperaturesByCity.csv*, which consists of temperature readings over time by city.

* These data sets were produced by [Berkeley Earth](www.berkelyearth.org), and are also available on [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).


```r
# Get Climate Data
country &lt;- read.csv("Data/GlobalLandTemperaturesByCountry.csv")
```

---
# Analysing Country Temperature Data

```r
head(country, 3)
```

```
##           dt AverageTemperature AverageTemperatureUncertainty Country
## 1 1743-11-01              4.384                         2.294   Åland
## 2 1743-12-01                 NA                            NA   Åland
## 3 1744-01-01                 NA                            NA   Åland
```


---
# Analysing country Temperature Data

```r
tail(country, 3)
```

```
##                dt AverageTemperature AverageTemperatureUncertainty
## 577460 2013-07-01             17.000                         0.453
## 577461 2013-08-01             19.759                         0.717
## 577462 2013-09-01                 NA                            NA
##         Country
## 577460 Zimbabwe
## 577461 Zimbabwe
## 577462 Zimbabwe
```

---
# Analysing country Temperature Data

* &lt;code&gt;mutate&lt;/code&gt; can be used to add new variables (columns) to a data frame 
by transforming existing variables

* Example syntax: &lt;code&gt;mutate(data, new.var = f(old.var))&lt;/code&gt;


```r
country &lt;- mutate(country, date = as.Date(dt))
head(country, 3)
```

```
##           dt AverageTemperature AverageTemperatureUncertainty Country
## 1 1743-11-01              4.384                         2.294   Åland
## 2 1743-12-01                 NA                            NA   Åland
## 3 1744-01-01                 NA                            NA   Åland
##         date
## 1 1743-11-01
## 2 1743-12-01
## 3 1744-01-01
```

---
# Analysing Temperature Data

```r
country &lt;- mutate(country, Temp_zscore = scale(AverageTemperature)[, 1])

ggplot(country, aes(Temp_zscore, fill=1)) + 
    geom_density(alpha=0.4)
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-46-1.png" width="672" /&gt;

---
# Analysing Temperature Data
Plot average temperatures for Germany over time.

```r
country &lt;- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %&gt;%
    filter(Country == "Germany") %&gt;%
    mutate(date = as.Date(dt))

ggplot(country, aes(date, AverageTemperature)) +
 geom_line()
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-47-1.png" width="672" /&gt;

---
# Analysing Temperature Data
Dificult to see the patterns, need to smooth, what value to choose?


```r
# Now using %&gt;%
k &lt;- ???
country &lt;- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %&gt;%
    filter(Country == "Germany") %&gt;%
    mutate(date = as.Date(dt))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

---
# Analysing Temperature Data
Monthly data, so window size 12 seems to make sense.


```r
# Now using %&gt;%
k &lt;- 12
country &lt;- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %&gt;%
    filter(Country == "Germany") %&gt;%
    mutate(date = as.Date(dt)) %&gt;%
    mutate(SmoothedAvg = stats::filter(AverageTemperature,
                                       rep(1/k, k), sides=2))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-49-1.png" width="672" /&gt;

---
# Analysing Temperature Data
Let's try 120 (10 years) as well.


```r
# Now using %&gt;%
k &lt;- 120
country &lt;- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %&gt;%
    filter(Country == "Germany") %&gt;%
    mutate(date = as.Date(dt)) %&gt;%
    mutate(SmoothedAvg = stats::filter(AverageTemperature,
                                       rep(1/k, k), sides=2))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

&lt;img src="Topics_In_R_DPluta_files/figure-html/unnamed-chunk-50-1.png" width="672" /&gt;

---
# Temperature Data Exercises

1. Find the mean temperature for each country and print the data frame with countries 
ordered from hottest to coolest (use &lt;code&gt;arrange&lt;/code&gt;).

2. Plot the temperature time series for a few different countries.  Compare different countries
that you think might be interesting, e.g. very populated vs sparsely populated countries, 
or by latitude and longitude.



```r
city &lt;- read.csv("Data/GlobalLandTemperaturesByCity.csv")
head(city)
city %&gt;% group_by(city) %&gt;% summarise(mean(AverageTemperature, na.rm=TRUE))
```


---
# References and Resources

* [Why Vectorise?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)
* [Writing Better Statistical Programs in R](http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/)
* [Advanced R](http://adv-r.had.co.nz/)
* [RStudio Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
* [&lt;code&gt;magrittr&lt;/code&gt; Vignette](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)
* [Kaggle Climate Change Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)
* [&lt;code&gt;dplyr&lt;/code&gt; Tutorial](http://genomicsclass.github.io/book/pages/dplyr_tutorial.html)
* [&lt;code&gt;dplyr&lt;/code&gt; Introduction Vignette](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)
* [Evaluating the Design of R](http://r.cs.purdue.edu/pub/ecoop12.pdf)
* [Vectorisation for Simulations](https://www.r-bloggers.com/how-to-use-vectorization-to-streamline-simulations/)
* [Bootstrapping with &lt;code&gt;broom&lt;/code&gt;](https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html)
* [R Markdown Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)
* [Strategies to Speedup R](https://www.r-bloggers.com/strategies-to-speedup-r-code/)


---

class: center, middle

# Thanks!

![](Figures/data_blob.png)

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github"
});</script>

  </body>
</html>
