---
title: "Topics in R Workshop"
subtitle: "UCI Data Science Initiative"
author: "Dustin Pluta"
date: "2 Dec 2016"
email: "dpluta@uci.edu"
output:
  xaringan::moon_reader:
    seal: yes
    
    nature:
      highlightStyle: github
---

![](Figures/UCI_logo.png)

![](Figures/dsi_logo.png)

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r, include=FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, message=FALSE)
```

---
# Topics in R: Afternoon Schedule

12:45 - 2:30pm: <code>dplyr</code>

2:40 - 3:50pm: Profiling and vectorisation

![](Figures/kiwi.jpg)

---
# Required Packages
Install needed packages:
```{r, eval=FALSE}
# setwd(~/source/file/location/)
install.packages('ggplot2')
install.packages('dplyr')
install.packages('broom')
install.packages('microbenchmark')
install.packages('devtools')
devtools::install_github('hadley/lineprof')
```

```{r, results='hide'}
library(ggplot2)
library(dplyr)
library(broom)
library(microbenchmark)
library(lineprof)
```


---

# Topics


1. Using <code>dplyr</code> Functions

--

2. Examples with <code>%>%</code>

--

3. Performant Code

--

4. Vectorisation

--

5. Profiling


---

# Some Resources for R

* [<code>dplyr</code> and Data Wrangling Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

* [Data Carpentry Lessons for R](http://www.datacarpentry.org/R-ecology-lesson/)

* [<code>dplyr</code> Tutorial](http://genomicsclass.github.io/book/pages/dplyr_tutorial.html)

* [Advanced R](http://adv-r.had.co.nz/)

* [R for Data Science](http://r4ds.had.co.nz/)

* [Coursera Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)

---

# <code>dplyr</code>

* <code>dplyr</code> is a package designed for easy and efficient data manipulation

* Fast enough to work with large(ish) data sets

* *tidyverse* philosophy: collection of small, simple functions that each do one thing well

* Written by Hadley Wickham, Chief Scientist for R Studio, who also developed:

    + <code>ggplot2</code>

    + <code>reshape2</code>

    + <code>tidyr </code>

    + many others
    
* **Note** Because Dr. Wickham is from New Zealand, we will be strictly using 
New Zealand spellings (colour, summarise, vectorisation, etc.) here.


---
# Key Functions

* <code> filter</code>: select subset of rows (observations)

* <code> select</code>: select subset of columns (variables)

* <code> mutate</code>: transform variables in a data set

* <code> arrange</code>: reorder rows

* <code> summarise</code>: collapses a data frame into a single row

* <code>group_by</code>

![](Figures/iris.png)

---
# Simple <code>dplyr</code> Examples
* Let's try some <code>dplyr</code> functions with the <code>iris</code> data set:
```{r}
# Manipulate the data.frame iris and print species means of Sepal Width
data(iris)
iris <-  filter(iris, Species!="setosa")
iris <- select(iris, c(Sepal.Width, Species)) 
iris <- group_by(iris, Species)
species_means <- summarise(iris, mean(Sepal.Width))
print(species_means)
```

* Note that each function has the form <code>f(data, ...)</code>: first 
argument is a data frame, second argument indicates what 
to do with the data frame.
* Each function is designed to perform a single specific type of operation on a data frame.
* Data can be cleaned and organized by chaining these operations together.

---
# Introducing <code>%>%</code>
* <code>dplyr</code> (and much of the *tidyverse*) is designed around the use of 
the pipe operator <code>%>%</code>

* The pipe operator <code>%>%</code> allows you to chain operations on a data set together 
without having to create specific intermediate objects

* When using <code>%>%</code>, the first argument to a function is taken as the output of the previous step in the chain

* For example, the following is equivalent to the previous code:
```{r}
# Prints species means, does not save anything
# Original data.frame iris is unaffected
data(iris)
iris %>% filter(Species!="setosa") %>%
    select(c(Sepal.Width, Species)) %>% 
    group_by(Species) %>%
    summarise(mean(Sepal.Width))
```

```{r, eval=FALSE}
# To save the results instead
species_means <- iris %>% 
    filter(Species!="setosa") %>%
    select(c(Sepal.Width, Species)) %>% 
    group_by(Species) %>%
    summarise(mean(Sepal.Width))
```

---
# <code>dplyr</code> Exercises

1. Use <code>dplyr</code> to calculate the mean Sepal Width of the virginica species.

2. <code>summarise</code> can summarise multiple variables simultaneously, applying a (possibly different) 
function to each variable.  
Adapt the code below to find the minimum, median, 
maximum, and standard deviation of the Sepal.Width for the virginica species.

3. <code>group_by()</code> makes <code>summarise</code> even more useful by allowing you 
to summarise values across groups of a category simultaneously.  
Using <code>group_by</code>, adapt your code from the previous problem to produce the summary values for each species.


**Modify this code for problems 2 and 3:**
```{r, eval=FALSE}
data(iris)
iris %>% summarise(mean_sepal_width = mean(Sepal.Width),
                   min_sepal_width = min(Sepal.Width))
```

---
# <code>dplyr</code> Exercises
*Solution*
* 1. Use <code>dplyr</code> to calculate the mean Sepal Width of the virginica species.

```{r}
data(iris)

iris %>% 
    filter(Species == "virginica") %>%
    summarise(mean_sepal_width = mean(Sepal.Width))
```


---
# <code>dplyr</code> Exercises
*Solution*

* 2. <code>summarise</code> can summarise multiple variables simultaneously, applying a (possibly different) 
function to each variable.  
Adapt the code below to find the minimum, median, 
maximum, and standard deviation of the Sepal.Width for the virginica species.


```{r}
data(iris)
iris %>% 
    filter(Species == "virginica") %>%
    summarise(min_sepal_width = min(Sepal.Width),
              med = median(Sepal.Width), maximum = max(Sepal.Width),
              stdev = sd(Sepal.Width))
```


---
# <code>dplyr</code> Exercises
*Solution*

* 3. <code>group_by()</code> makes <code>summarise</code> even more useful by allowing you 
to summarise values across groups of a category simultaneously.  
Using <code>group_by</code>, adapt your code from the previous problem to produce the summary values for each species.


```{r}
data(iris)
iris %>% 
    group_by(Species) %>%
    summarise(min_sepal_width = min(Sepal.Width),
              med = median(Sepal.Width), maximum = max(Sepal.Width),
              stdev = sd(Sepal.Width))
```


---
class: center, middle
# Performant Code

>“We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.”
~ Donald Knuth.

![](Figures/young-donald-knuth.jpg)


---
# Performant Code

* When writing code, there are many different resources to consider:
    + Speed
    + Memory Usage
    + Developer Time
    + Code Reusability

* We generally want to make the data analysis process easy, convenient, and reliable, 
it's rarely necessary to write the fastest code possible.

* Goal: learn easy methods and tools to reduce the time and burden of programming 
for data analysis.
    + Vectorisation
    + Profiling tools
    + Design principles

---
# Performant Code

**Procedures** (from [Advanced R](adv-r.had.co.na/Profiling.html#improve-perf])):

1. Identify the bottlenecks in a script with profiling tools.
    + <code>microbenchmark</code>
    + <code>lineprof</code>
2. Look for existing solutions.
    + Published packages
    + StackOverflow
3. Vectorise.
    + <code> apply, lapply </code>
    + Specialized functions, e.g. <code>colSums, rowSums</code>
4. Parallelise.
5. Avoid copying code uncessarily; avoid superfluous copies of data and variables.
6. Byte-code compile.
7. Rewrite in a faster language (C++, Rcpp, Julia).


---
# Vectorisation

* R is **bad** with "native" <code>for</code> loops, due to various aspects of 
R's design

* Vectorised functions in R tend to be much faster than equivalent loops.

* The underlying loops of the vectorised fucntions are implemented in a lower level 
language like C, which can greatly improve runtimes.

* Key functions:
    + **<code>apply(X, MARGIN, FUN)</code>:** Apply function FUN to matrix/array X, across the dimension(s) specified by MARGIN.
    
    + **<code>sapply(X, FUN)</code>:** Apply a function to a list of vectors, lists, or similar, and returns a vector or matrix by default.

---
# Vectorisation: Iris Example

Suppose we want to calculate the mean of all the numeric variables in the <code>iris</code> 
data set.

```{r}
data(iris)
head(iris)
# With a loop:
iris_means <- c()
for (i in 1:4) {
    iris_means[i] <- mean(iris[, i])
}

# Vectorised:
iris_means <- apply(iris[, -5], 2, mean)
```

---
# Vectorisation: Iris Example
Let's use vectorisation to calculate 95% bootstrap confidence intervals for the 
median Sepal.Length for each iris species.

**Part A**
First, use <code>group_by</code> and <code>summarise</code> to calculate the median Sepal Length of each species.  Then plot density/histogram plots of the distributions 
of Sepal Lengths for the three species using <code>ggplot</code>.

---
# Vectorisation: Iris Example
**Part A**
*Solution*

```{r, fig.height=2.6}
iris_medians <- group_by(iris, Species) %>%
    summarise(median(Sepal.Length))

ggplot(iris, aes(Sepal.Length, fill=Species)) + 
    geom_density(alpha = 0.3) + 
    geom_vline(aes(xintercept = iris_medians[, 2]))
```

---
# Vectorisation: Iris Example
**Part B**

* Suppose we want to print the lower and upper 95\% bootstrap estimates, along with the sample median Sepal Length.  

    + Clean up and rewrite the following bootstrap code using vectorisation and <code>dplyr</code> functions. 
    
    (Hint: the <code>boostrap()</code> function from <code>broom</code> makes bootstrap sampling easy.)

---
# Vectorisation: Iris Example
**Part B**

Modify this code using <code>dplyr</code> functions.
```{r, eval=FALSE}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- subset(iris, Species == "setosa")
boot_samples <- matrix(nrow=B, ncol=n)
boot_medians <- c()
for (b in 1:B) {
    boot_samples[b, ] <- sample_n(setosa, n, replace=TRUE)[, "Sepal.Length"] 
    boot_medians[b] <- median(boot_samples[b, ])
}

boot_median_est <- mean(boot_medians)
boot_lower <- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper <- boot_medians[order(boot_medians)][round(0.975*B)]
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Use the <code>boostrap()</code> function.
```{r, eval=FALSE}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- subset(iris, Species == "setosa")
boot_samples <- matrix(nrow=B, ncol=n)
boot_medians <- c()

boot_samples <- bootstrap(setosa, B)



boot_median_est <- mean(boot_medians)
boot_lower <- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper <- boot_medians[order(boot_medians)][round(0.975*B)]
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Replace <code>subset</code> with <code>filter</code>, and remove extraneous declarations.
```{r, eval=FALSE}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- filter(iris, Species == "setosa")

boot_samples <- bootstrap(setosa, B)
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate medians for each replicate using <code>do</code> and <code>tidy</code>.
```{r, eval=FALSE}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- filter(iris, Species == "setosa")

boot_samples <- bootstrap(setosa, B)
boot_medians <- boot_samples %>% do(tidy(median(.$Sepal.Length)))
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate bootstrap confidence bounds for the median Sepal Length.
```{r}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- filter(iris, Species == "setosa")

boot_samples <- bootstrap(setosa, B)
boot_medians <- boot_samples %>% do(tidy(median(.$Sepal.Length)))
boot_bounds <- quantile(boot_medians$x, probs=c(0.025, 0.975))

cat("Est Median Sepal Length: ", mean(boot_medians$x), "\n", 
    "95% Bootstrap CI for Median Sepal Length: (", 
    boot_bounds[1], ",", boot_bounds[2], ")\n")
```


---
# Vectorisation: Iris Example
**Part B**
*Solution*

Calculate bootstrap confidence bounds for the median Sepal Length.
```{r}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
boot_medians <- iris %>% 
    filter(Species == "setosa") %>%
    bootstrap(B) %>% 
    do(tidy(median(.$Sepal.Length)))
boot_bounds <- quantile(boot_medians$x, probs=c(0.025, 0.975))
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

Compare back to what we started with.
```{r, eval=FALSE}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
setosa <- subset(iris, Species == "setosa")
boot_samples <- matrix(nrow=B, ncol=n)
boot_medians <- c()
for (b in 1:B) {
    boot_samples[b, ] <- sample_n(setosa, n, replace=TRUE)[, "Sepal.Length"] 
    boot_medians[b] <- median(boot_samples[b, ])
}

boot_median_est <- mean(boot_medians)
boot_lower <- boot_medians[order(boot_medians)][round(0.025*B)]
boot_upper <- boot_medians[order(boot_medians)][round(0.975*B)]
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

<code>dplyr</code> version again, now with printing for results.
```{r}
data(iris)
set.seed(1234)
B <- 1000 # Number of bootstrap samples
n <- 50 # There's 50 of each species in the data set

# This code calculates bootstrap samples for just "setosa"
boot_medians <- iris %>% 
    filter(Species == "setosa") %>%
    bootstrap(B) %>% 
    do(tidy(median(.$Sepal.Length)))
boot_bounds <- quantile(boot_medians$x, probs=c(0.025, 0.975))

cat("Est Median Sepal Length: ", mean(boot_medians$x), "\n", 
    "95% Bootstrap CI for Median Sepal Length: (", 
    boot_bounds[1], ",", boot_bounds[2], ")\n")
```

---
# Vectorisation: Iris Example
**Part B**
*Solution*

```{r, fig.height=3}
ggplot(boot_medians, aes(x)) + 
    geom_histogram(binwidth=0.05)
```

*End iris example*

---
# Profiling

**<code>lineprof</code>** is a package for profiling R scripts.

Here is an example on a simple function, from [Advanced R](adv-r/had/co.nz/Profiling.html)

```{r}
library(lineprof)
f <- function() {
  pause(0.1)
  g()
  h()
}
g <- function() {
  pause(0.1)
  h()
}
h <- function() {
  pause(0.1)
}

L <- lineprof(f())
L
```

---
# Profiling

```{r, eval=FALSE}
# lm runs too fast on small data to test with lineprof
data(iris)
lineprof(lm(Sepal.Length ~ Species, data=iris))
```

```{r}
# Simulate some bigger data to test lm
N <- 1E4
beta <- c(1, 0, 2, -3, 4)
X <- as.matrix(rnorm(N*length(beta), 10, 1), nrow=N)
Y <- X%*%beta + rnorm(N, 0, 2)
L <- lineprof(lm(Y ~ X))
L
```

---
# Profiling
* The profiling package <code>microbenchmark</code> can also be used to profile 
small pieces of code.

* <code>microbenchmark</code> will run input code many times to try to get stable 
estimates of run times, thus it can take very long to run for complicated or long-running 
functions.

```{r}
library(microbenchmark)
microbenchmark(lm(Sepal.Length ~ Species, data=iris))
microbenchmark(lm(Y ~ X))
```

---
# Profiling: Vectorisation

Let's compare the speeds of for-loops and the equivalent vectorisations.

```{r}
set.seed(1234)
n <- 100
a <- rnorm(n)
b <- rexp(n, 5)

dot_loop <- function(a, b) {
    result <- 0
    for (i in 1:n)
        result <- result + a[i]*b[i]
    return(result)
}
dot_loop(a, b)
a%*%b
```

---
# Profiling: Vectorisation
```{r}
microbenchmark(dot_loop(a, b))
microbenchmark(a%*%b)
```

---
# Profiling: Vectorisation
```{r}
set.seed(1234)
n <- 1E5
a <- rnorm(n, 10, 2)
b <- rexp(n, 5)

microbenchmark(dot_loop(a, b))
microbenchmark(a%*%b)
```

---
# Profiling: Vectorisation
```{r, eval=FALSE}
# This size may take a minute or two to finish
set.seed(1234)
n <- 1E6
a <- rnorm(n, 10, 2)
b <- rexp(n, 5)

microbenchmark(dot_loop(a, b))
microbenchmark(a%*%b)
```

---
# Profiling: Vector Sum Example

* Let's compare summing a vector with a for-loop vs using <code>sum</code>.

* Write two functions to return the sum of all entries of an input vector *x*: 
    
    + <code>loop_sum(x)</code> using a for-loop
    
    + <code>vect_sum(x)</code> using vectorisation (i.e., with <code>sum()</code>)


---
# Profiling: Vector Sum Example
*Solution*

* Let's compare summing a vector with <code>sum</code> vs a for-loop.

* Write two functions to return the sum of all entries of an input vector *x*: 
   
    + <code>loop_sum(x)</code> using a for-loop
   
    + <code>vect_sum(x)</code> using vectorised functions (Hint: <code>sum</code>)

```{r}
loop_sum <- function(x) {
    result <- 0
    for (i in 1:length(x)) {
        result <- result + x[i]
    }
    return(result)
}

vect_sum <- function(x) {
    return(sum(x))
}
```

---
# Profiling: Vector Sum Example
```{r}
set.seed(1234)
x <- rnorm(500)

microbenchmark(loop_sum(x))
microbenchmark(vect_sum(x))
```

---
# Profiling: <code>aggregate</code> vs <code>dplyr</code>

Let's compare using <code>aggregate</code> to <code>dplyr</code> for 
finding the standard deviation of Sepal Length for each of the iris species.

```{r}
data(iris)

microbenchmark(aggregate(Sepal.Length ~ Species, data=iris, FUN=mean))

microbenchmark(iris %>% 
                   group_by(Species) %>% 
                   summarise(mean(Sepal.Length)))

microbenchmark(iris %>% 
                   select(Sepal.Length, Species) %>%
                   group_by(Species) %>% 
                   summarise(mean(Sepal.Length)))
```

---
# Profiling: <code>aggregate</code> vs <code>dplyr</code>

```{r, eval=FALSE}
country <- read.csv("Data/countryLandTemperaturesByCountry.csv")

microbenchmark(aggregate(AverageTemperature ~ Country, data=country, FUN=mean))
microbenchmark(country %>% group_by(Country) %>% 
                   summarise(mean(AverageTemperature)))
```

---
# Profiling: Iris Example

To profile the previous code from the Iris Example, we wrap everything 
in a function:
```{r}
set.seed(1234)
data(iris)

iris_loop <- function(B, n=50,
                           species="setosa", var="Sepal.Length") {
    iris_species <- subset(iris, Species == species)
    boot_samples <- matrix(nrow=B, ncol=n)
    boot_medians <- c()
    for (b in 1:B) {
        boot_samples[b, ] <- sample_n(setosa, n, replace=TRUE)[, var] 
        boot_medians[b] <- median(boot_samples[b, ])
    }
    boot_median_est <- mean(boot_medians)
    boot_lower <- boot_medians[order(boot_medians)][round(0.025*B)]
    boot_upper <- boot_medians[order(boot_medians)][round(0.975*B)]
    return(c(boot_median_est, boot_lower, boot_upper))
}
iris_loop(50)
```

---
# Profiling: Iris Example
```{r, eval=FALSE}
microbenchmark(iris_loop(300))
lineprof(iris_loop(1000))
```

---
# Profiling: Iris Example

Now for the <code>dplyr</code>.
```{r}
set.seed(1234)
iris_vect <- function(B, n=50, species="setosa") {
    boot_medians <- iris %>% 
        filter(Species == species) %>%
        bootstrap(B) %>% 
        do(tidy(median(.$Sepal.Length)))
    boot_bounds <- quantile(boot_medians$x, probs=c(0.025, 0.975))
    return(c(mean(boot_medians$x), boot_bounds))
}
iris_vect(50)
```

---
# Profiling: Iris Example

Comparing the two versions for the bootstrap iris estimates.

```{r}
microbenchmark(iris_loop(300))
microbenchmark(iris_vect(300))
```


---
# Profiling: Iris Example

Comparing the two versions for the bootstrap iris estimates.

```{r}
lineprof(iris_loop(2000))
lineprof(iris_vect(2000))
```

---
# Profiling Exercise

1. Suppose we want to find the column sums of the numeric variables in the <code>iris</code> data set.  
Use <code>microbenchmark</code> to compare the speed of three methods for computing 
the column sums:
    
    + double for-loop over columns and rows
    
    + <code>apply</code> and <code>sum</code>
    
    + <code>colSums</code>
    
2. Simulate a random matrix (say 100 x 100 or more), and benchmark the different column 
sum methods.

---
# Profiling Exercise
*Solution*

```{r}
data(iris)

loop <- function(dat) {
    sums <- c()
    for (j in 1:4) {
        result <- 0
        for (i in nrow(dat)) {
            result <- result + iris[i, j]
        }
        sums <- c(sums, result)
    }
    return(sums)
}

my_apply <- function(dat) {
   return(apply(dat, 2, sum))
}

my_colsums <- function(dat)
    return(colSums(dat))
```

---
# Profiling Exercise
*Solution*

```{r}
microbenchmark(loop(iris[, -5]))
microbenchmark(my_apply(iris[, -5]))
microbenchmark(my_colsums(iris[, -5]))
```

---
# Profiling Exercise
*Solution*

```{r}
set.seed(1234)
n_col <- 100
n_row <- 100
X <- matrix(rnorm(n_col*n_row), nrow=n_row)

microbenchmark(loop(X))
microbenchmark(my_apply(X))
microbenchmark(my_colsums(X))
```

---
# Summary

* Vectorization and good code design and save you lots of time in the long run, and 
gets much easier with practice.

* When applicable, vectorization tends to be much faster than for-loops

* However, some care must be taken in correctly implementing the vectorized code

* Loops may make more sense when
    + using functions that do not take vectors as input
    + computations where one iteration depends on previous iterations

* If you can't use built-in vertorised functions and have to use a loop, 
the package <code>Rcpp</code> can be used to write fast for-loops in C++.

---
# Summary

* There is a nice R-bloggers post on [Strategies to Speedup R Code](https://www.r-bloggers.com/strategies-to-speedup-r-code/), 
which includes many plots showing relative speedups for different 
strategies.

<img src="Figures/raw_vs_with_vectorization.png" alt="vectorization" style="width: 600px;"/>


---
# Language Speed Comparisons
* For situations where speed and efficiency is critical, you will likely need to 
resort to other languages:

<img src="Figures/julia_benchmarks.png" alt="Language Comparisons" style="width: 700px;"/>


---
# Analysing Climate Data

* Let's apply some of the ideas we've seen to more complicated real-world data sets.

* This is a more open-ended section, to give you an opportunity to explore and practice 
using <code>dplyr</code>, vectorisation, and profiling.

* Download climate data *GlobalTemperatures.csv*, which consists of temperature readings globally over time.

* Download climate data *GlobalLandTemperaturesByCountry.csv*, which consists of temperature readings over time by country.

* Download climate data *GlobalLandTemperaturesByCity.csv*, which consists of temperature readings over time by city.

* These data sets were produced by [Berkeley Earth](www.berkelyearth.org), and are also available on [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).

```{r, warning=FALSE}
# Get Climate Data
country <- read.csv("Data/GlobalLandTemperaturesByCountry.csv")
```

---
# Analysing Country Temperature Data
```{r}
head(country, 3)
```


---
# Analysing country Temperature Data
```{r}
tail(country, 3)
```

---
# Analysing country Temperature Data

* <code>mutate</code> can be used to add new variables (columns) to a data frame 
by transforming existing variables

* Example syntax: <code>mutate(data, new.var = f(old.var))</code>

```{r}
country <- mutate(country, date = as.Date(dt))
head(country, 3)
```

---
# Analysing Temperature Data
```{r, warning=FALSE, fig.height=4}
country <- mutate(country, Temp_zscore = scale(AverageTemperature)[, 1])

ggplot(country, aes(Temp_zscore, fill=1)) + 
    geom_density(alpha=0.4)
```

---
# Analysing Temperature Data
Plot average temperatures for Germany over time.
```{r, warning=FALSE, fig.height=2.3}
country <- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %>%
    filter(Country == "Germany") %>%
    mutate(date = as.Date(dt))

ggplot(country, aes(date, AverageTemperature)) +
 geom_line()
```

---
# Analysing Temperature Data
Dificult to see the patterns, need to smooth, what value to choose?

```{r, eval=FALSE}
# Now using %>%
k <- ???
country <- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %>%
    filter(Country == "Germany") %>%
    mutate(date = as.Date(dt))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

---
# Analysing Temperature Data
Monthly data, so window size 12 seems to make sense.

```{r, fig.height=2.3, warning=FALSE, message=FALSE}
# Now using %>%
k <- 12
country <- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %>%
    filter(Country == "Germany") %>%
    mutate(date = as.Date(dt)) %>%
    mutate(SmoothedAvg = stats::filter(AverageTemperature,
                                       rep(1/k, k), sides=2))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

---
# Analysing Temperature Data
Let's try 120 (10 years) as well.

```{r, fig.height=2.3}
# Now using %>%
k <- 120
country <- read.csv("Data/GlobalLandTemperaturesByCountry.csv") %>%
    filter(Country == "Germany") %>%
    mutate(date = as.Date(dt)) %>%
    mutate(SmoothedAvg = stats::filter(AverageTemperature,
                                       rep(1/k, k), sides=2))

ggplot(country, aes(date, SmoothedAvg)) +
    geom_line()
```

---
# Temperature Data Exercises

1. Find the mean temperature for each country and print the data frame with countries 
ordered from hottest to coolest (use <code>arrange</code>).

2. Plot the temperature time series for a few different countries.  Compare different countries
that you think might be interesting, e.g. very populated vs sparsely populated countries, 
or by latitude and longitude.


```{r, eval=FALSE}
city <- read.csv("Data/GlobalLandTemperaturesByCity.csv")
head(city)
city %>% group_by(city) %>% summarise(mean(AverageTemperature, na.rm=TRUE))
```


---
# References and Resources

* [Why Vectorise?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)
* [Writing Better Statistical Programs in R](http://www.johnmyleswhite.com/notebook/2013/01/24/writing-better-statistical-programs-in-r/)
* [Advanced R](http://adv-r.had.co.nz/)
* [RStudio Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
* [<code>magrittr</code> Vignette](https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html)
* [Kaggle Climate Change Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)
* [<code>dplyr</code> Tutorial](http://genomicsclass.github.io/book/pages/dplyr_tutorial.html)
* [<code>dplyr</code> Introduction Vignette](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)
* [Evaluating the Design of R](http://r.cs.purdue.edu/pub/ecoop12.pdf)
* [Vectorisation for Simulations](https://www.r-bloggers.com/how-to-use-vectorization-to-streamline-simulations/)
* [Bootstrapping with <code>broom</code>](https://cran.r-project.org/web/packages/broom/vignettes/bootstrapping.html)
* [R Markdown Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)
* [Strategies to Speedup R](https://www.r-bloggers.com/strategies-to-speedup-r-code/)


---

class: center, middle

# Thanks!

![](Figures/data_blob.png)

Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).
