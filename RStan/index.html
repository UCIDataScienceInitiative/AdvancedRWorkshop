<!DOCTYPE html>
<html>
<head>
  <title>RStan</title>
  <meta charset="utf-8">
  <meta name="description" content="RStan">
  <meta name="author" content="Sepehr Akhavan">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/logo.css"></link>
<link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="assets/img/logo.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>RStan</h1>
    <h2>UCI Data Science Initiative</h2>
    <p>Sepehr Akhavan<br/>Dept. of Statistics</p>
  </hgroup>
    <a href="https://github.com/UCIDataScienceInitiative/AdvancedRWorkshop/zipball/gh-pages" class="example">
     Download
    </a>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Agenda</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Brief introduction to Bayesian Analysis</li>
<li>Available software for Bayesian Analysis</li>
<li>Bayesian Analysis using RStan</li>
<li>Examples</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Bayesian Analysis</h2>
  </hgroup>
  <article data-timings="">
    <p>Baye&#39;s Theorem:</p>

<p>\[ P(\theta|Y) = \frac{P(Y | \theta) P(\theta)}{P(Y)}\]</p>

<p>Where:</p>

<ul>
<li>\(\theta\) : unknown parameter(s)</li>
<li>\(Y\) : observed data</li>
</ul>

<p>We can write the formula above as:
\[ P(\theta|Y) \propto P(Y | \theta) P(\theta) = P(\theta, Y)\]</p>

<p><strong>Goal</strong> : To characterize the posterior distribution \(P(\theta|Y)\)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Bayesian Analysis</h2>
  </hgroup>
  <article data-timings="">
    <p>Posterior distribution \(P(\theta|Y)\) :</p>

<ul>
<li>1) may have a closed form (ex. conjugate prior)</li>
<li>2) USUALLY doesn&#39;t have a closed form and is not analytically tractable! </li>
</ul>

<p>if (2), How to generate samples from \(P(\theta|Y)\) ?</p>

<ul>
<li><p>solution 1) MCMC: a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. </p>

<ul>
<li>ref:<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&amp;rep=rep1&amp;type=pdf">An Introduction to MCMC for Machine Learning</a></li>
</ul></li>
<li><p>solution 2) Variational Bayes: Approximate the posterior with a simpler distribution.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Bayesian Analysis</h2>
  </hgroup>
  <article data-timings="">
    <p>Some examples of MCMC methods:</p>

<ul>
<li>Metropolisâ€“Hastings algorithm</li>
<li>Gibbs sampling</li>
<li>Slice sampling</li>
<li>Reversible-jump</li>
<li>Hamiltonian Monte Carlo

<ul>
<li><a href="http://www.mcmchandbook.net/HandbookChapter5.pdf">MCMC Using Hamiltonian Dynamics</a></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Some Bayesian Analysis Packages:</h2>
  </hgroup>
  <article data-timings="">
    <p>1) WinBUGS/OpenBUGS:</p>

<ul>
<li><a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/">http://www.mrc-bsu.cam.ac.uk/software/bugs/</a></li>
</ul>

<p>2) JAGS:</p>

<ul>
<li><a href="http://mcmc-jags.sourceforge.net">http://mcmc-jags.sourceforge.net</a></li>
</ul>

<p>3) Stan:</p>

<ul>
<li><a href="http://mc-stan.org">http://mc-stan.org</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>What is Stan?</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>&quot;a modeling language in which statisticians could write their models in familiar notation that could be transformed to efficient C++ code and then compiled into an efficient executable program.&quot;</p></li>
<li><p>Stan uses HMC method for sampling as opposed to Gibbs sampling.</p></li>
<li><p>Stan automatically adapt the number of steps during HMC sampling using No-U-Turn (NUTS) sampler:</p>

<ul>
<li>ref: Hoffman and Gelman, 2011, 2014</li>
</ul></li>
<li><p>Stan&#39;s interfaces include: RStan, PyStan, CmdStan, ...</p></li>
<li><p>to install stan, please check:</p>

<ul>
<li><a href="https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started">https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started</a></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Where to get help?</h2>
  </hgroup>
  <article data-timings="">
    <p>1) Stan User&#39;s Guide &amp; Reference Manual:</p>

<ul>
<li>Current version: 2.8.0</li>
<li>You can access it <a href="https://github.com/stan-dev/stan/releases/download/v2.8.0/stan-reference-2.8.0.pdf">here</a></li>
</ul>

<p>2)  Stan User&#39;s group:</p>

<ul>
<li><a href="https://groups.google.com/forum/#!forum/stan-users">https://groups.google.com/forum/#!forum/stan-users</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Simple Posterior Example</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Consider an example of flipping a coin N times where \(P(head) = \theta\):
\[P(Y | \theta) \propto \theta^Y (1 - \theta)^{(N - Y)}\]</p></li>
<li><p>Now, suppose \(\theta\) (probability of head) is unknown. We can assume a prior of the form:
\[\theta \sim Beta(\alpha, \beta)\]</p></li>
<li><p>Under this setting, posterior of \(\theta\) would be of the form:
\[P(\theta|Y) \propto P(Y | \theta) P(\theta)\]</p></li>
<li><p>It&#39;s then easy to show:
\[P(\theta|Y) \propto \theta^{(\alpha + Y - 1)} (1 - \theta)^{(\beta + (N - Y) - 1)}\]
\[P(\theta|Y) \propto Beta\big(\alpha + Y, \beta + (N - Y) \big)\]</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Simple Posterior Example</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Life is easy when posterior distribution is tractable :)</li>
</ul>

<pre><code class="r">N &lt;- 200
theta.true &lt;- 0.4
Y &lt;- rbinom(1, size=N, prob = 0.4)
alpha &lt;- 2
beta &lt;- 2

PostSample &lt;- rbeta(10000, shape1 = alpha + Y,  shape2 = beta + (N - Y))

# Posterior Mean:
mean(PostSample)
</code></pre>

<pre><code>## [1] 0.3778661
</code></pre>

<pre><code class="r"># 95% CR:
quantile(PostSample, probs = c(0.025, 0.975))
</code></pre>

<pre><code>##      2.5%     97.5% 
## 0.3131006 0.4442364
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Simple Posterior Example in Stan</h2>
  </hgroup>
  <article data-timings="">
    <p>There are 3 steps needed to implement this model in Stan:</p>

<p>1) Writing the model in Stan</p>

<p>2) Initializing the data for the model</p>

<p>3) Running the model</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Simple Posterior Example in Stan (Stan Model)</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  int&lt;lower = 1&gt; N;
  int&lt;lower = 0, upper = N&gt; Y;
  real&lt;lower = 0&gt; alpha;
  real&lt;lower = 0&gt; beta;
}
parameters {
  real&lt;lower = 0, upper = 1&gt; theta;
}
model {
  theta ~ beta(alpha, beta);
  Y ~ binomial(N, theta);
}
</code></pre>

<ul>
<li>Good practice to save your stan model as a .txt or .stan into a file.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Simple Posterior Example in Stan</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r"># We already have N, Y,alpha, and beta
data = list(N = N, Y = Y, alpha = alpha, beta = beta)
fit &lt;- stan(&quot;SimpleCoinEx.stan&quot;, data = data, chains = 1, iter = 1000, warmup = 200)
summary(fit)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Bayesian Modeling with Stan (R section)</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>stan(): does all of the work of fitting a Stan model and returning the results. It includes:</p>

<ul>
<li>1) translating the Stan model to C++ code</li>
<li>2) Then, the C++ code is compiled into a binary shared object, which is loaded into the current R session </li>
<li>3) Finally, samples are drawn and wrapped in an object</li>
</ul></li>
<li><p>stan(): can also be used to sample again from a fitted model under different settings (e.g., different iter).</p>

<ul>
<li>To do so, fit argument should be provided</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Bayesian Modeling with Stan (R section)</h2>
  </hgroup>
  <article data-timings="">
    <p>Useful functions to try on an stan fit:</p>

<ul>
<li><p>summary()</p></li>
<li><p>print()</p></li>
<li><p>traceplot()</p></li>
<li><p>extract()</p></li>
<li><p>launch_shinystan{shinystan}</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Bayesian Modeling with Stan (Stan Model)</h2>
  </hgroup>
  <article data-timings="">
    <p>The rest of the workshop will be on details of writing stan models. In particular, we will learn more on:</p>

<ul>
<li><p>Stan&#39;s Program Blocks</p></li>
<li><p>Data Types, Variable Declaration, and Statements</p></li>
<li><p>Operators &amp; Built-In Functions:</p></li>
<li><p>User-Defined Functions</p></li>
<li><p>Reparameterization and Change of Variables</p></li>
<li><p>Truncated Data</p></li>
<li><p>Censored Data</p></li>
<li><p>Examples </p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Stan&#39;s Program Blocks</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">functions{
}
data {
}
transformed data {
}
parameters {
}
transformed parameters {
}
model {
}
generated quantities{ 
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h3>function block:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>This optional block contains user-defined functions (if there is any)</p></li>
<li><p>more on this later</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h3>data Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>All variables that are read in as data are declared here</p></li>
<li><p>Data are read only once!</p></li>
<li><p>The data block does not allow statements</p></li>
<li><p>Input data should jibe with the declared data in this block:</p>

<ul>
<li>within the same range</li>
<li>having the same size</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h3>transformed data Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Defining variables that are fixed when running the program</p></li>
<li><p>No reading from external sources</p></li>
<li><p>Variables defined earlier in the data block may be used here</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-20" style="background:;">
  <hgroup>
    <h3>parameters Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Variables defined in this block are sampled</p></li>
<li><p>Variables defined as parameters cannot be directly assigned</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-21" style="background:;">
  <hgroup>
    <h3>transformed parameters Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>There is no need to perform a direct sampling for variables declared under this block</p></li>
<li><p>Variables that are defined in terms of data or transformed data only (fixed) should be defined in the transformed data block. Those variables are illegal under this block</p></li>
<li><p>Variables defined here will be written as part of the output</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-22" style="background:;">
  <hgroup>
    <h3>model Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>likelihood (the model/distribution the data come from) is defined under this block</p></li>
<li><p>priors on the parameters are also defined in this block</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-23" style="background:;">
  <hgroup>
    <h3>generated quantities Block</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>This block is executed only after a sample has been generated</p></li>
<li><p>Therefore, nothing in the generated quantities block affects the sampled parameter values</p></li>
<li><p>Some applications of this block are:</p>

<ul>
<li>to calculate posterior expectations</li>
<li>transforming parameters for reporting</li>
<li>generating predictions for new data</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-24" style="background:;">
  <hgroup>
    <h2>Data Types and Variable Declarations:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Variables should have a declared data type. Stan&#39;s data types include:</p></li>
<li><p>1) Primitive Data Types:</p>

<ul>
<li>1.1) real</li>
<li>1.2) int</li>
<li>1.3) constrained by using lower or upper</li>
</ul></li>
<li><p>2) Compound Data Types:</p>

<ul>
<li>2.1) vector (of real values)</li>
<li>2.2) row_vector (of real values)</li>
<li>2.3) matrix (of real values)</li>
<li>2.4) constrained include: simplex, unit_vector, ordered, positive_ordered, corr_matrix, cov_matrix, cholesky_factor_cov, cholesky_factor_cor</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-25" style="background:;">
  <hgroup>
    <h2>Data Types and Variable Declarations:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>3) Arrays:

<ul>
<li>3.1) three-dimensional arrays of integers</li>
<li>3.2) four-dimensional arrays of row_vectors</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-26" style="background:;">
  <hgroup>
    <h2>Data Types and Variable Declarations:</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r"># primitive:
int&lt;lower = 1&gt; N;
real&lt;lower = 0, upper = 1&gt; theta;

# vector:
vector[3] myVec;
vector&lt;lower = 0&gt;[3] myConsVec;
simplex[5] theta;

# matrix:
matrix[3,3] A;
corr_matrix[3] Sigma;
cholesky_factor_corr[5] L;

# array:
int n[5]; // n is an array of 5 integers
real a[3,4];
vector[7] mu[3]; // 3-dimensional array of vectors of length 7
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-27" style="background:;">
  <hgroup>
    <h2>Statements:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Assignment Statements</p></li>
<li><p>Sampling Statement</p></li>
<li><p>Log Probability Increment Statement</p></li>
<li><p>For Loops</p></li>
<li><p>Conditional Statements</p></li>
<li><p>Print</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-28" style="background:;">
  <hgroup>
    <h3>Assignment Statements:</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">n &lt;- 1;
Sigma[1,1] &lt;- 1;
</code></pre>

<h3>Sampling Statements:</h3>

<pre><code class="r">y ~ normal(mu, sigma);
</code></pre>

<h3>Log Probability Increment Statement:</h3>

<p>The statement above is equivalent to:</p>

<pre><code class="r">increment_log_prob(normal_log(y, mu, sigma))
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-29" style="background:;">
  <hgroup>
    <h3>For Loops:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Suppose N is an integer and y is a an array of real values of length N:</li>
</ul>

<pre><code class="r">for (n in 1:N){
  y[n] ~ normal(mu, sigma);
}
</code></pre>

<ul>
<li>alternatively, we can use vectorized operations in stan and write:</li>
</ul>

<pre><code class="r">y ~ normal(mu, sigma)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-30" style="background:;">
  <hgroup>
    <h3>Conditional Statements:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Stan supports if-then-else syntax as in C++:</li>
</ul>

<pre><code class="r">if (condition1)
  statement1
else if (condition2)
  statement2
//...
else if (conditionN)
  statementN
else
  elseStatement
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-31" style="background:;">
  <hgroup>
    <h3>Print:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Stan does not have a stepwise debugger</p></li>
<li><p>Instead, we can use the traditional debug-by-printt method !</p></li>
<li><p>For instance, to print the value of variables y and z, use:</p></li>
</ul>

<pre><code class="r">print(&quot;y=&quot;, y, &quot; z=&quot;, z);
</code></pre>

<ul>
<li><p>Print statements may be used anywhere other statements may be used</p></li>
<li><p>frequency depends on how often the block they are in is evaluated</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-32" style="background:;">
  <hgroup>
    <h3>Print:</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">transformed data {
  matrix[2,2] u;
  u[1,1] &lt;- 1.0;  u[1,2] &lt;- 4.0;
  u[2,1] &lt;- 9.0;  u[2,2] &lt;- 16.0;
  for (n in 1:2)
    print(&quot;u[&quot;, n, &quot;] = &quot;, u[n]);
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-33" style="background:;">
  <hgroup>
    <h2>Operators:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Logical Operators:</p>

<ul>
<li>||, &amp;&amp;, ==, !=, &lt;, &lt;=, &gt;, &gt;=</li>
</ul></li>
<li><p>Arithmatic Operators:</p>

<ul>
<li class='*,'>./: element-wise matrix operators</li>
<li>&#39;: to get transpose of a matrix</li>
<li>For a complete list of operators, please visit the User&#39;s manual

<ul>
<li>Page 306 - User&#39;s Manual 2.8.0 !</li>
</ul></li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-34" style="background:;">
  <hgroup>
    <h2>Built-In Functions:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Stan includes many built-in functions. Some examples are:</p>

<ul>
<li>determinant(): determinant of a matrix</li>
<li>inverse(): inverse of a matrix</li>
<li>inverse_spd(): The inverse of A where A is symmetric, positive definite.</li>
<li>eigenvalues_sym(): returns the vector of eigenvalues of a symmetric matrix A in ascending order</li>
<li>For more info, please read &quot;Built-in Functions&quot; in the User&#39;s Manual 2.8.0.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-35" style="background:;">
  <hgroup>
    <h2>User-Defined Functions:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Stan allows users to define their own functions</p></li>
<li><p>User defined functions appear in the function block and before all other blocks</p></li>
<li><p>function definition has C++ style where the type of the output as well as arguments should be explicitely mentioned:</p></li>
</ul>

<pre><code class="r">real myFunc(real arg1, real arg2){
  // ... statements
}
</code></pre>

<ul>
<li>functions with no output have type &quot;void&quot;:</li>
</ul>

<pre><code class="r">void myFunc(real arg1, real arg2){
  // ... statements
}
</code></pre>

<ul>
<li>You can learn more about functions in section 26.1 (page 282) of the <a href="https://github.com/stan-dev/stan/releases/download/v2.8.0/stan-reference-2.8.0.pdf">user manual</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-36" style="background:;">
  <hgroup>
    <h2>Reparameterization and Change of Variables:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Stan supports a direct encoding of reparameterizations.</p></li>
<li><p>Stan also supports changes of variables. To do so:</p>

<ul>
<li>We should directly increment the log probability accumulator with the log Jacobian of the transform</li>
</ul></li>
<li><p>Why we care about reparameterization?</p>

<ul>
<li>different parameterizations sometime have different computational performances</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-37" style="background:;">
  <hgroup>
    <h3>Reparameterization:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>This can be best understood with an example below (Stan user&#39;s manual):</p></li>
<li><p>Conisder Beta distribution with two different parameterizations:</p>

<ul>
<li>1) \(Beta(\alpha, \beta)\): \(\alpha\) and \(\beta\) are shape parameters</li>
<li>2) \(Beta(\phi, \lambda)\): 

<ul>
<li>2.1) \(\phi = \alpha/(\alpha + \beta)\) (mean)</li>
<li>2.2) \(\lambda = \alpha + \beta\)</li>
</ul></li>
</ul></li>
<li><p>Under this section, we specify y&#39;s (data) from the same \(Beta(\alpha, \beta)\) under both reparameterization (no need for Jacobian!). </p></li>
<li><p>However, in (1), we directly sample \(\alpha\) and \(\beta\), where in (2), we first sample \(\phi\) and \(\lambda\) and we compute \(\alpha\) and \(\beta\) accordingly</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-38" style="background:;">
  <hgroup>
    <h3>Reparameterization (1st Parameterization):</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  vector&lt;lower = 0, upper = 1&gt;[N] theta;
}
parameters {
  real&lt;lower = 0&gt; alpha;
  real&lt;lower = 0&gt; beta;
  ...
}
model {
  alpha ~ ... ; // our prior for alpha
  beta ~ ... ; // our prior for beta
  theta ~ beta(alpha, beta); 
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-39" style="background:;">
  <hgroup>
    <h3>Reparameterization (2nd Parameterization):</h3>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">parameters {
  real &lt;lower = 0, upper = 1&gt; phi;
  real &lt;lower = 0&gt; lambda;
  ...
}
transformed parameters{
  real&lt;lower = 0&gt; alpha;
  real&lt;lower = 0&gt; beta;
  alpha &lt;- lambda*phi;
  beta &lt;- lambda*(1 - phi);
}
model {
  phi ~ ... ; // prior for phi
  lambda ~ ...; // prior for lambda
  y ~ beta(alpha, beta); // transformed param used. No Jacobian Needed!
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-40" style="background:;">
  <hgroup>
    <h3>Change of Variables:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Consider the distribution of variable \(\theta > 0\) where it&#39;s log is normally distributed:</li>
</ul>

<p>\[log(\theta) \sim N(\mu, \sigma)\]</p>

<ul>
<li><p>Then we know \(\theta\) is distributed as:
\[P(\theta) = N(log(\theta) | \mu, \sigma) \times |\frac{d}{d\theta} log(\theta)|\]
\[P(\theta) = N(log(\theta) | \mu, \sigma) 1/\theta\]</p></li>
<li><p>Stan works with log probability, so:
\[log(P(\theta)) = log \big(Normal(log(\theta) | \mu, \sigma)\big) - log(\theta)\]</p></li>
<li><p>so we need to incremenet the likelihood by:</p>

<ul>
<li>log absolute derivative (ex. \(log (|\frac{d}{d\theta} log(\theta)|)\))</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-41" style="background:;">
  <hgroup>
    <h3>Change of Variables:</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li>We can then easily implement the model as:</li>
</ul>

<pre><code class="r">parameters {
  real&lt;lower = 0&gt; theta;
}
model{
  log(theta) ~ normal(mu, sigma);
  increment_log_prob(-log(theta));
  ...
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-42" style="background:;">
  <hgroup>
    <h3>The distinction between Reparam. vs. Change of Var?</h3>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Reparam.: We first sample a parameter and then we transform it.</p></li>
<li><p>Change of Var: We first transform a variable, and then we sample it!</p></li>
<li><p>only the latter needs Jacobian!</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-43" style="background:;">
  <hgroup>
    <h2>Truncated Data:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Trucated data: measurements are only reported if they fall with a boundary</p></li>
<li><p>Truncation in Stan is restricted to:</p>

<ul>
<li>univariate distributions</li>
<li>with available log cumulative distribution</li>
</ul></li>
<li><p>To understand truncation better, you can consider a detector which only is activated if the signals it detects are above a certain limit. </p></li>
<li><p>There may be lots of weak incoming signals, but we can never tell using this detector.</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-44" style="background:;">
  <hgroup>
    <h2>Truncated Data:</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  int&lt;lower=0&gt; N;
  real U;
  real&lt;upper=U&gt; y[N];
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  for (n in 1:N)
    y[n] ~ normal(mu,sigma) T[,U];
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-45" style="background:;">
  <hgroup>
    <h2>Censored Data:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Under censoring, some observations may be censored, meaning that we only know that they are below (or above) some bound but we don&#39;t know their actual values.</p></li>
<li><p>Unlike with truncated data, the number of data points that were censored is known.</p></li>
<li><p>It&#39;s wrong to ignore censored data!</p></li>
<li><p>One way to model censored data is to treat the censored data as missing data that is constrained to fall in the censored range of values. </p></li>
<li><p>Note that Stan does not allow unknown values in its arrays or matrices</p></li>
<li><p>So we can treat censored data as unknown parameters!</p></li>
<li><p>In that case, censored data will be sampled like other parameters</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-46" style="background:;">
  <hgroup>
    <h2>Censored Data:</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_cens;
  real y_obs[N_obs];
  real&lt;lower=max(y_obs)&gt; U;
}
parameters {
  real&lt;lower=U&gt; y_cens[N_cens];
  real mu;
  real&lt;lower=0&gt; sigma;
} 
model {
  y_obs ~ normal(mu,sigma);
  y_cens ~ normal(mu,sigma);
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-47" style="background:;">
  <hgroup>
    <h2>Censored Data - Likelihood Approach:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>it is not necessary to impute values.</p></li>
<li><p>Instead, the values can be integrated out:</p></li>
<li><p>Each censored data point has a probability of:
\[Pr[Y > U] = \int_{U}^{\infty} P(Y| ...) dY = 1 - cdf(U)\]</p></li>
<li><p>the complementary CDF above in Stan is of the form:</p>

<ul>
<li>distName_ccdf_log</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-48" style="background:;">
  <hgroup>
    <h2>Censored Data - Likelihood Approach:</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  int&lt;lower=0&gt; N_obs;
  int&lt;lower=0&gt; N_cens;
  real y_obs[N_obs];
  real&lt;lower=max(y_obs)&gt; U;
}
parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
}
model {
  y_obs ~ normal(mu,sigma);
  increment_log_prob(N_cens * normal_ccdf_log(U,mu,sigma));
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-49" style="background:;">
  <hgroup>
    <h2>Exercise1 - Revisiting the Coin Flip example:</h2>
  </hgroup>
  <article data-timings="">
    <h3>Borrowed from Daniel Lee (@djsyclik)</h3>

<pre><code class="r">data {
  int N;
  int Y[N];
}
parameters {
  real&lt;lower=0, upper=1&gt; theta;
} 
model {
  Y ~ bernoulli(theta);
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-50" style="background:;">
  <hgroup>
    <h2>Exercise1 - Revisiting the Coin Flip example:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Alternatively, we can specify the model using the log likelihood approach:</li>
</ul>

<pre><code class="r">...
model {
  for (n in 1:N) 
    increment_log_prob(bernoulli_log(Y[n], theta));
}
</code></pre>

<ul>
<li>Instead of using bernoulli_log() function in Stan, you can explicitely specify the log likelihood as:</li>
</ul>

<pre><code class="r">...
model {
  for (n in 1:N) 
    increment_log_prob(log(if_else(y[n] == 1, theta, 1-theta)));
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-51" style="background:;">
  <hgroup>
    <h2>Exercise1 - Revisiting the Coin Flip example:</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Finally, you can use vectorized operations so you can get rid of loops:</li>
</ul>

<pre><code class="r">...
model {
  increment_log_prob(bernoulli_log(Y, theta));
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-52" style="background:;">
  <hgroup>
    <h2>Exercise2 - Zero-Inflated Poisson</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>source: User&#39;s Manual - Page 108</p></li>
<li><p>Consider a zero-inflated poisson distribution where:</p>

<ul>
<li>with probability \(\theta\), we draw a zero</li>
<li>with probability \(1 - \theta\), we draw from \(Poisson(\lambda)\)</li>
<li>this means:
\[p(y_n | \theta, \lambda) = \begin{cases}
\theta + (1 - \theta) \times Poisson(0|\lambda) & \text{if }y_n = 0\\
(1 - \theta) \times Poisson(y_n|\lambda) & \text{if }y_n \ne 0\\
\end{cases}\]</li>
<li>How to implement this in stan?</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-53" style="background:;">
  <hgroup>
    <h2>Exercise2 - Zero-Inflated Poisson (Solution)</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="r">data {
  int&lt;lower = 0&gt; N;
  int&lt;lower = 0&gt; Y[N];
}
parameters {
  real&lt;lower = 0, upper = 1&gt; theta;
  real&lt;lower = 0&gt; lambda;
}
model {
  theta ~ beta(1, 1);
  lambda ~ cauchy(0, 5);

  for (n in 1:N) {
    if (Y[n] == 0)
      increment_log_prob(log_sum_exp(bernoulli_log(1, theta),
                                     bernoulli_log(0, theta)
                                     + poisson_log(Y[n], lambda) ));
    else
      increment_log_prob(bernoulli_log(0, theta)
                         + poisson_log(Y[n], lambda) );
  }
}
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-54" style="background:;">
  <hgroup>
    <h2>Exercise3: Simple Linear Regression</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Consider a model of the form:
\[y_i = \alpha + \beta x_i + \epsilon_i\]</p></li>
<li><p>step 1) simulate some data for the model above</p></li>
<li><p>step 2) fit a linear regression in R (frequentist model)</p></li>
<li><p>step 3) fit a Bayesian linear regression using Stan</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-55" style="background:;">
  <hgroup>
    <h2>Exercise4: Finite Mixtures</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>Suppose Y belongs to a finite mixture of distributions.</p></li>
<li><p>This means eahc outcome \(Y_i\) is drawn from one those distributions</p></li>
<li><p>In paticular, suppose Y belongs to a mixture of K normal distributions. </p></li>
<li><p>Consider \(\theta\) as a K-simplex. </p></li>
<li><p>Per each \(Y_i\), there is a corresponding latent variable \(z_i \in \{ 1, 2, \dots, K\}\)</p></li>
<li><p>We can then write:
\[Z_i \sim Categorical(\theta)\]
\[y_i \sim Normal(\mu_{z_i}, \sigma)\]</p></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-56" style="background:;">
  <hgroup>
    <h2>Exercise4: Finite Mixtures</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li><p>By summing over the latent \(Z_i\)&#39;s, we get:
\[P_Y(y | \theta, \mu, \sigma) = \sum_{k = 1}^{K} \theta_k Normal(\mu_k, \sigma)\]</p></li>
<li><p>Consider a mixutre of two Normal distributions: N(-2.5, 1), N(2.5, 1)</p></li>
<li><p>Consider mixing proportion as: \(\theta = (0.4, 0.6)\)</p></li>
<li><p>For this problem:</p>

<ul>
<li>step1) simulate data</li>
<li>step2) run a Bayesian model using stan</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Agenda'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Bayesian Analysis'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Bayesian Analysis'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Bayesian Analysis'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Some Bayesian Analysis Packages:'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='What is Stan?'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Where to get help?'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Simple Posterior Example'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Simple Posterior Example'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Simple Posterior Example in Stan'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Simple Posterior Example in Stan (Stan Model)'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Simple Posterior Example in Stan'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Bayesian Modeling with Stan (R section)'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Bayesian Modeling with Stan (R section)'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Bayesian Modeling with Stan (Stan Model)'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Stan&#39;s Program Blocks'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='function block:'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='data Block'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='transformed data Block'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='parameters Block'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='transformed parameters Block'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='model Block'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='generated quantities Block'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='Data Types and Variable Declarations:'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='Data Types and Variable Declarations:'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Data Types and Variable Declarations:'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='Statements:'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='Assignment Statements:'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='For Loops:'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='Conditional Statements:'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='Print:'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='Print:'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='Operators:'>
         33
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=34 title='Built-In Functions:'>
         34
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=35 title='User-Defined Functions:'>
         35
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=36 title='Reparameterization and Change of Variables:'>
         36
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=37 title='Reparameterization:'>
         37
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=38 title='Reparameterization (1st Parameterization):'>
         38
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=39 title='Reparameterization (2nd Parameterization):'>
         39
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=40 title='Change of Variables:'>
         40
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=41 title='Change of Variables:'>
         41
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=42 title='The distinction between Reparam. vs. Change of Var?'>
         42
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=43 title='Truncated Data:'>
         43
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=44 title='Truncated Data:'>
         44
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=45 title='Censored Data:'>
         45
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=46 title='Censored Data:'>
         46
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=47 title='Censored Data - Likelihood Approach:'>
         47
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=48 title='Censored Data - Likelihood Approach:'>
         48
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=49 title='Exercise1 - Revisiting the Coin Flip example:'>
         49
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=50 title='Exercise1 - Revisiting the Coin Flip example:'>
         50
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=51 title='Exercise1 - Revisiting the Coin Flip example:'>
         51
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=52 title='Exercise2 - Zero-Inflated Poisson'>
         52
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=53 title='Exercise2 - Zero-Inflated Poisson (Solution)'>
         53
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=54 title='Exercise3: Simple Linear Regression'>
         54
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=55 title='Exercise4: Finite Mixtures'>
         55
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=56 title='Exercise4: Finite Mixtures'>
         56
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>